
## 19장 - 강화 학습으로 복잡한 환경에서 의사 결정하기


### 목차

- 경험에서 배운다
    - 강화 학습 이해
    - 강화 학습 시스템의 에이전트-환경 인터페이스 정의하기
- 강화 학습의 기초 이론
    - 마르코프 결정 과정
    - 마르코프 결정 과정의 수학 공식
        - 마르코프 과정 시각화
        - 에피소드 작업 대 연속적인 작업
    - 강화 학습 용어: 대가, 정책, 가치 함수
        - 대가
        - 정책
        - 가치 함수
    - 벨먼 방정식을 사용한 동적 계획법
- 강화 학습 알고리즘
    - 동적 계획법
        - 정책 평가 - 동적 계획법으로 가치 함수 예측하기
        - 추정된 가치 함수로 정책 향상시키기
        - 정책 반복
        - 가치 반복
    - 몬테 카를로를 사용한 강화 학습
        - MC를 사용한 상태-가치 함수 추정
        - MC를 사용한 행동-가치 함수 추정
        - MC 제어를 사용해 최적의 정책 찾기
        - 정책 향상 - 행동-가치 함수로부터 그리디 정책 계산하기
    - 시간차 학습
        - TD 예측
        - 온-폴리시 TD 제어 (SARSA)
        - 오프-폴리시 TD 제어 (Q-러닝)
- 첫 번째 강화 학습 알고리즘 구현하기
    - OpenAI 짐 툴킷 소개
        - OpenAI 짐에 포함된 환경 사용하기
        - 그리드 월드
        - OpenAI 짐에서 그리드 월드 환경 구현하기
    - Q-러닝으로 그리드 월드 문제 풀기
        - Q-러닝 알고리즘 구현하기
    - 심층 Q-러닝
        - Q-러닝 알고리즘에 따라 DQN 모델 훈련하기
            - 재생 메모리
            - 손실 계산을 위해 타깃 가치 결정하기
    - 심층 Q-러닝 알고리즘 구현
- 전체 요약

**코드를 실행하기 위한 자세한 정보는 [`../ch01`](../ch01)에 있는 [README.md](../ch01/README.md) 파일을 참고하세요.**
