## 16장 트랜스포머 - 어텐션 메커니즘을 통한 자연어 처리 성능 향상

### 목차

- 어텐션 메커니즘이 추가된 RNN
  - RNN의 정보 검색을 돕는 어텐션
  - RNN을 위한 원본 어텐션 메커니즘
  - 양방향 RNN으로 입력 처리하기
  - 문맥 벡터에서 출력 생성하기
  - 어텐션 가중치 계산하기
- 셀프 어텐션 메커니즘 소개
  - 기본적인 형태의 셀프 어텐션
  - 훈련 가능한 셀프 어텐션 메카니즘: 스케일드 점곱 어텐션
- 어텐션이 필요한 전부다: 원본 트랜스포머 아키텍처
  - 멀티 헤드 어텐션으로 문맥 임베딩 인코딩하기
  - 언어 모델 학습: 디코더와 마스크드 멀티 헤드 어텐션
  - 구현 세부 사항: 위치 인코딩 및 층 정규화
- 레이블이 없는 데이터를 활용하여 대규모 언어 모델 구축
  - 트랜스포머 모델 사전 훈련 및 미세 튜닝
  - GPT로 레이블이 없는 데이터 활용하기
  - GPT-2를 사용하여 새로운 텍스트 생성
  - BERT를 통한 양방향 사전 훈련
  - 두 장점을 합친 BART
- 파이토치에서 BERT 모델 미세 튜닝하기
  - IMDb 영화 리뷰 데이터셋 로드
  - 데이터셋 토큰화
  - 사전 훈련된 BERT 모델 로드 및 미세 튜닝하기
  - 트레이너 API를 사용하여 트랜스포머를 간편하게 미세 튜닝하기
- 요약

**코드를 실행하기 위한 자세한 정보는 [`../ch01`](../ch01)에 있는 [README.md](../ch01/README.md) 파일을 참고하세요.**

