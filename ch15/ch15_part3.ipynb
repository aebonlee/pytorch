{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ZFf3hWFcIu7t"
      },
      "source": [
        "# 머신 러닝 교과서 - 파이토치편"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxbQJLUzIu7w"
      },
      "source": [
        "<table align=\"left\"><tr><td>\n",
        "<a href=\"https://colab.research.google.com/github/rickiepark/ml-with-pytorch/blob/main/ch15/ch15_part3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"코랩에서 실행하기\"/></a>\n",
        "</td></tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2jS-agIIu7x"
      },
      "source": [
        "## 패키지 버전 체크"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUmm5b3Iu7x"
      },
      "source": [
        "check_packages.py 스크립트에서 로드하기 위해 폴더를 추가합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uCo8gOexIu7x",
        "outputId": "863d5a0e-bc8a-45d0-a2e7-52170f12572b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-05 08:00:39--  https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/python_environment_check.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1629 (1.6K) [text/plain]\n",
            "Saving to: ‘python_environment_check.py’\n",
            "\n",
            "\r          python_en   0%[                    ]       0  --.-KB/s               \rpython_environment_ 100%[===================>]   1.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-09-05 08:00:39 (28.6 MB/s) - ‘python_environment_check.py’ saved [1629/1629]\n",
            "\n",
            "--2023-09-05 08:00:40--  https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/1268-0.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1171600 (1.1M) [text/plain]\n",
            "Saving to: ‘1268-0.txt’\n",
            "\n",
            "1268-0.txt          100%[===================>]   1.12M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-09-05 08:00:46 (55.2 MB/s) - ‘1268-0.txt’ saved [1171600/1171600]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# 코랩의 경우 깃허브 저장소로부터 python_environment_check.py를 다운로드 합니다.\n",
        "if 'google.colab' in sys.modules:\n",
        "    !wget https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/python_environment_check.py\n",
        "    !wget https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/1268-0.txt\n",
        "else:\n",
        "    sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WUpT7C4Iu7y"
      },
      "source": [
        "권장 패키지 버전을 확인하세요:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eau3ABn3Iu7y",
        "outputId": "99a27dc8-2688-4814-a9eb-3460330a4d20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Your Python version is 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0]\n",
            "[OK] torch 2.0.1+cu118\n"
          ]
        }
      ],
      "source": [
        "from python_environment_check import check_packages\n",
        "\n",
        "\n",
        "d = {\n",
        "    'torch': '1.8.0',\n",
        "}\n",
        "check_packages(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Yxf5TOIu7z"
      },
      "source": [
        "# 15장 - 순환 신경망으로 순차 데이터 모델링 (파트 3/3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOznh67rIu70"
      },
      "source": [
        "**목차**\n",
        "\n",
        "- 파이토치로 시퀀스 모델링을 위한 RNN 구현\n",
        "  - 두 번째 프로젝트: 텐서플로로 글자 단위 언어 모델 구현\n",
        "    - 데이터셋 전처리\n",
        "    - 문자 수준의 RNN 모델 만들기\n",
        "    - 평가 단계: 새로운 텍스트 생성\n",
        "- 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jL7BFbKTIu70"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D92PkQtIu70"
      },
      "source": [
        "## 두 번째 프로젝트: 텐서플로로 글자 단위 언어 모델 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OSiX99uoIu70",
        "outputId": "82c3e562-132b-4e0b-9c51-28a914e67b18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_11.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_11.png', width=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnY_wM0QIu70"
      },
      "source": [
        "### 데이터셋 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vZLETk7qIu71",
        "outputId": "f517b268-0434-4e8b-dd58-dbf993361405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 길이: 1112350\n",
            "고유한 문자: 80\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "## 텍스트 읽고 전처리하기\n",
        "with open('1268-0.txt', 'r', encoding=\"utf8\") as fp:\n",
        "    text=fp.read()\n",
        "\n",
        "start_indx = text.find('THE MYSTERIOUS ISLAND')\n",
        "end_indx = text.find('End of the Project Gutenberg')\n",
        "\n",
        "text = text[start_indx:end_indx]\n",
        "char_set = set(text)\n",
        "print('전체 길이:', len(text))\n",
        "print('고유한 문자:', len(char_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JZx2RoOyIu71",
        "outputId": "826f72ce-c971-410a-ecff-346fd478c8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_12.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_12.png', width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hvdS_TvqIu71",
        "outputId": "afb3ed71-311c-4d0a-90f1-e7ba4cfa24ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "인코딩된 텍스트 크기:  (1112350,)\n",
            "THE MYSTERIOUS       == 인코딩 ==>  [44 32 29  1 37 48 43 44 29 42 33 39 45 43  1]\n",
            "[33 43 36 25 38 28]  == 디코딩  ==>  ISLAND\n"
          ]
        }
      ],
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch:i for i,ch in enumerate(chars_sorted)}\n",
        "char_array = np.array(chars_sorted)\n",
        "\n",
        "text_encoded = np.array(\n",
        "    [char2int[ch] for ch in text],\n",
        "    dtype=np.int32)\n",
        "\n",
        "print('인코딩된 텍스트 크기: ', text_encoded.shape)\n",
        "\n",
        "print(text[:15], '     == 인코딩 ==> ', text_encoded[:15])\n",
        "print(text_encoded[15:21], ' == 디코딩  ==> ', ''.join(char_array[text_encoded[15:21]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1uOdvroDIu71",
        "outputId": "279d139a-7909-400a-8768-8b44f626ab90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44 -> T\n",
            "32 -> H\n",
            "29 -> E\n",
            "1 ->  \n",
            "37 -> M\n"
          ]
        }
      ],
      "source": [
        "for ex in text_encoded[:5]:\n",
        "    print('{} -> {}'.format(ex, char_array[ex]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jsOxuLoYIu71",
        "outputId": "22b3a5d5-0548-410f-a815-ac8428ba514a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_13.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_13.png', width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JQDOhNrfIu72",
        "outputId": "4d3d87d8-db34-492f-9d39-cab7ab61e44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_14.png\" width=\"500\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/rickiepark/ml-with-pytorch/main/ch15/figures/15_14.png', width=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0hOjeE54Iu72",
        "outputId": "17f26668-090b-4a05-ee7d-1908b454484c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[44 32 29  1 37 48 43 44 29 42 33 39 45 43  1 33 43 36 25 38 28  1  6  6\n",
            "  6  0  0  0  0  0 40 67 64 53 70 52 54 53  1 51]  ->  74\n",
            "'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'  ->  'y'\n"
          ]
        }
      ],
      "source": [
        "seq_length = 40\n",
        "chunk_size = seq_length + 1\n",
        "\n",
        "text_chunks = [text_encoded[i:i+chunk_size]\n",
        "               for i in range(len(text_encoded)-chunk_size+1)]\n",
        "\n",
        "## 조사:\n",
        "for seq in text_chunks[:1]:\n",
        "    input_seq = seq[:seq_length]\n",
        "    target = seq[seq_length]\n",
        "    print(input_seq, ' -> ', target)\n",
        "    print(repr(''.join(char_array[input_seq])),\n",
        "          ' -> ', repr(''.join(char_array[target])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ImCA-iHOIu72",
        "outputId": "586e78d4-e21e-461e-9969-dd516bf81e0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-178b9785e257>:15: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  seq_dataset = TextDataset(torch.tensor(text_chunks))\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text_chunks):\n",
        "        self.text_chunks = text_chunks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_chunk = self.text_chunks[idx]\n",
        "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
        "\n",
        "seq_dataset = TextDataset(torch.tensor(text_chunks))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K2iHzdYQIu72",
        "outputId": "83084164-5f74-43a9-de7b-69d97ba568f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 (x): 'THE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced b'\n",
            "타깃 (y): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "\n",
            "입력 (x): 'HE MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by'\n",
            "타깃 (y): 'E MYSTERIOUS ISLAND ***\\n\\n\\n\\n\\nProduced by '\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, (seq, target) in enumerate(seq_dataset):\n",
        "    print('입력 (x):', repr(''.join(char_array[seq])))\n",
        "    print('타깃 (y):', repr(''.join(char_array[target])))\n",
        "    print()\n",
        "    if i == 1:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uFZVsNHWIu72"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "# device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Jq4KJNKEIu72"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "seq_dl = DataLoader(seq_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSGAxO5Iu73"
      },
      "source": [
        "### 문자 수준의 RNN 모델 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dcqbQphyIu73",
        "outputId": "b07e62f4-f77f-4fe9-fb54-b45e01b90905",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embedding): Embedding(80, 256)\n",
              "  (rnn): LSTM(256, 512, batch_first=True)\n",
              "  (fc): Linear(in_features=512, out_features=80, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn_hidden_size = rnn_hidden_size\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc = nn.Linear(rnn_hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embedding(x).unsqueeze(1)\n",
        "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
        "        out = self.fc(out).reshape(out.size(0), -1)\n",
        "        return out, hidden, cell\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
        "        return hidden.to(device), cell.to(device)\n",
        "\n",
        "vocab_size = len(char_array)\n",
        "embed_dim = 256\n",
        "rnn_hidden_size = 512\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size)\n",
        "model = model.to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LIQ1KQ7OIu73",
        "outputId": "4f532633-ac98-4b00-dcdf-e24cbf137ab7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에포크 0 손실: 4.3722\n",
            "에포크 500 손실: 1.3959\n",
            "에포크 1000 손실: 1.3312\n",
            "에포크 1500 손실: 1.2258\n",
            "에포크 2000 손실: 1.2467\n",
            "에포크 2500 손실: 1.1968\n",
            "에포크 3000 손실: 1.1577\n",
            "에포크 3500 손실: 1.1474\n",
            "에포크 4000 손실: 1.1845\n",
            "에포크 4500 손실: 1.1594\n",
            "에포크 5000 손실: 1.0981\n",
            "에포크 5500 손실: 1.1191\n",
            "에포크 6000 손실: 1.1446\n",
            "에포크 6500 손실: 1.1238\n",
            "에포크 7000 손실: 1.1057\n",
            "에포크 7500 손실: 1.1707\n",
            "에포크 8000 손실: 1.1454\n",
            "에포크 8500 손실: 1.1631\n",
            "에포크 9000 손실: 1.0950\n",
            "에포크 9500 손실: 1.1302\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "num_epochs = 10000\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    hidden, cell = model.init_hidden(batch_size)\n",
        "    seq_batch, target_batch = next(iter(seq_dl))\n",
        "    seq_batch = seq_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    loss = 0\n",
        "    for c in range(seq_length):\n",
        "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
        "        loss += loss_fn(pred, target_batch[:, c])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    loss = loss.item()/seq_length\n",
        "    if epoch % 500 == 0:\n",
        "        print(f'에포크 {epoch} 손실: {loss:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwasl-_fIu73"
      },
      "source": [
        "### 평가 단계: 새로운 텍스트 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PW8EKpLlIu73",
        "outputId": "877848a9-9b40-49df-de80-7743b939de65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확률: [0.33333334 0.33333334 0.33333334]\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 1.0]])\n",
        "\n",
        "print('확률:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VkDBsUqtIu73",
        "outputId": "c44dbb71-6ecf-4780-f4ac-14749ce95b88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "확률: [0.10650698 0.10650698 0.78698605]\n",
            "[[0]\n",
            " [2]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('확률:', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "m = Categorical(logits=logits)\n",
        "samples = m.sample((10,))\n",
        "\n",
        "print(samples.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fwFFj1HKIu73",
        "outputId": "060d4f97-53a0-4b51-b45a-b65c1e21ab39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island was begin and, as the lumma wild build is\n",
            "small walkon of animals; a bed to igration the sailor, but in well in protecting the spurs of the ocean hand,\n",
            "or being animals, smple; to have been dangerous exist asonish in agains of ever result or sistance in the intelligence, all the animal who, he escaped from the furnace of Pencroft, who accumulated the passage of these detaker’s prittering on the wreck of\n",
            "this temposeral or two to a Prosce from where their eyes ranks, had not a single cries of wh\n"
          ]
        }
      ],
      "source": [
        "def sample(model, starting_str,\n",
        "           len_generated_text=500,\n",
        "           scale_factor=1.0):\n",
        "\n",
        "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
        "    encoded_input = torch.reshape(encoded_input, (1, -1))\n",
        "\n",
        "    generated_str = starting_str\n",
        "\n",
        "    model.eval()\n",
        "    hidden, cell = model.init_hidden(1)\n",
        "    hidden = hidden.to('cpu')\n",
        "    cell = cell.to('cpu')\n",
        "    for c in range(len(starting_str)-1):\n",
        "        _, hidden, cell = model(encoded_input[:, c].view(1), hidden, cell)\n",
        "\n",
        "    last_char = encoded_input[:, -1]\n",
        "    for i in range(len_generated_text):\n",
        "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
        "        logits = torch.squeeze(logits, 0)\n",
        "        scaled_logits = logits * scale_factor\n",
        "        m = Categorical(logits=scaled_logits)\n",
        "        last_char = m.sample()\n",
        "        generated_str += str(char_array[last_char])\n",
        "\n",
        "    return generated_str\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model.to('cpu')\n",
        "print(sample(model, starting_str='The island'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lecW4dFUIu74"
      },
      "source": [
        "* **예측 가능성 vs. 무작위성**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ki61mzaTIu74",
        "outputId": "adc3190b-d55f-47df-c3c4-c2ccf7b3f28e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "스케일 조정 전의 확률:         [0.10650698 0.10650698 0.78698605]\n",
            "0.5배 조정 후 확률: [0.21194156 0.21194156 0.57611686]\n",
            "0.1배 조정 후 확률: [0.3104238  0.3104238  0.37915248]\n"
          ]
        }
      ],
      "source": [
        "logits = torch.tensor([[1.0, 1.0, 3.0]])\n",
        "\n",
        "print('스케일 조정 전의 확률:        ', nn.functional.softmax(logits, dim=1).numpy()[0])\n",
        "\n",
        "print('0.5배 조정 후 확률:', nn.functional.softmax(0.5*logits, dim=1).numpy()[0])\n",
        "\n",
        "print('0.1배 조정 후 확률:', nn.functional.softmax(0.1*logits, dim=1).numpy()[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true,
        "id": "c-qdDePVIu74",
        "outputId": "30e116e3-b9cf-4844-95fa-e0f88d81783b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island was an\n",
            "insure, and the sand would be seen their close of the house, and the intelligence and a straight of the surrounded on the shores of the island would have an abundant which would have been able to followed by the power.\n",
            "\n",
            "“No, my friend,” said Herbert, “and as if you, Pencroft and Herbert was that the first rivales were completed the operation of the colonists and his companions.\n",
            "\n",
            "Herbert, who was seen the colonists’ pottering of the coast of the colonists were honest for the convicts, the\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=2.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "scrolled": true,
        "id": "LszAuKeYIu74",
        "outputId": "2de6ccc0-d8a6-428a-868e-9c5336d6b0c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The island\n",
            "would bozent jemutterinuana rollum. “Lazzing ISLCalson of anihmic; Cape, I pier\n",
            "‘pinutnely,” Tide,” said Coolleguark: their arras; t miys by--nisaries,” he akyed Cyrtam,\n",
            "smple;\n",
            "to hain; in--nder, Cor exhaoial;, an inhago was deed--a hruck frosts, Jacriam clvinomes’, at lo,-koednoated, grar, he escen;\n",
            "Positm ton ”ickally\n",
            "severally drawn I am-Neb “or here, pierced how ly histe’s privatebk, we diok recemevin,\n",
            "leanging?” sighted, two\n",
            "CPratch fallewled! Yon.\n",
            "\n",
            "After an\n",
            "weakle nearl fifly Lur, will we\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1)\n",
        "print(sample(model, starting_str='The island',\n",
        "             scale_factor=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHjUA38pIu74"
      },
      "source": [
        "# 요약"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}